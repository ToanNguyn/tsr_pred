{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "157829f7",
   "metadata": {},
   "source": [
    "# I. Import libraries and load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cf793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (mean_absolute_error, mean_squared_error, r2_score)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52adabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 42\n",
    "    batch_size = 64\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    SMALL_GAP = 6 \n",
    "    lookback = 24\n",
    "    horizon = 2\n",
    "    epochs = 6\n",
    "    lr = 1e-3\n",
    "    models_dir = './models'\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "torch.manual_seed(CFG.seed)\n",
    "np.random.seed(CFG.seed)\n",
    "if CFG.device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(CFG.seed)\n",
    "\n",
    "MASK_VALUE = 0.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c44957",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f137e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_interstate_traffic_volume = fetch_ucirepo(id=492) \n",
    "  \n",
    "X = metro_interstate_traffic_volume.data.features \n",
    "y = metro_interstate_traffic_volume.data.targets  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb7074e",
   "metadata": {},
   "source": [
    "# II. Sanity check and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ba8805",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = X.copy()\n",
    "df['traffic_volume'] = y.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f92d3c",
   "metadata": {},
   "source": [
    "### Check columns data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a16fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727a2df2",
   "metadata": {},
   "source": [
    "## 1. Transform date_time to datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544b923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date_time\"] = pd.to_datetime(df[\"date_time\"])\n",
    "df = df.sort_values(\"date_time\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2422721d",
   "metadata": {},
   "source": [
    "## 2. Drop duplicated rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6171d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicates().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651e5be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceabc82",
   "metadata": {},
   "source": [
    "### Check featuers statistic and distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3832fe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653b87eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = ['temp', 'rain_1h', 'snow_1h', 'clouds_all', 'traffic_volume']\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 15))\n",
    "axes = axes.flatten() \n",
    "\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    df.boxplot(column=col, ax=axes[i]) \n",
    "    axes[i].set_title(f'Box Plot of {col}', fontsize=14)\n",
    "    axes[i].set_ylabel(col, fontsize=12)\n",
    "        \n",
    "if len(numerical_cols) < len(axes):\n",
    "    for j in range(len(numerical_cols), len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "plt.suptitle('Outlier Visualization using Box Plots', fontsize=18, y=1.02)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 1.0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d70c44",
   "metadata": {},
   "source": [
    "## 3. Handle invalid data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc87e32",
   "metadata": {},
   "source": [
    "### temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ddbd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['temp'] = df['temp'].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0394e8b6",
   "metadata": {},
   "source": [
    "### rain_1h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edf9ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_max_rain = df.loc[df['rain_1h'] < 100, 'rain_1h'].max()\n",
    "df.loc[df['rain_1h'] > 100, 'rain_1h'] = real_max_rain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2f027b",
   "metadata": {},
   "source": [
    "## 4. Handle mising value in holiday column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954ceee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['holiday'] = df['holiday'].fillna('None')\n",
    "df['is_holiday'] = (df['holiday'] != 'None').astype(int)\n",
    "df = df.drop(columns=[\"holiday\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f8e31b",
   "metadata": {},
   "source": [
    "## 5. Gộp timestamp bị trùng and handle missing time stamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9abb55",
   "metadata": {},
   "source": [
    "### Gộp duplicated timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bf8b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = df.copy()\n",
    "\n",
    "numeric_cols = [\"temp\", \"rain_1h\", \"snow_1h\", \"clouds_all\", \"traffic_volume\"]\n",
    "cate_col = ['weather_description', 'weather_main']\n",
    "first_cols = [\"is_holiday\"]\n",
    "\n",
    "# Hàm lấy Mode an toàn (trả về giá trị đầu tiên nếu có nhiều mode)\n",
    "def get_mode(x):\n",
    "    m = pd.Series.mode(x)\n",
    "    return m.iloc[0] if not m.empty else np.nan\n",
    "\n",
    "df_grouped = df.groupby(\"date_time\").agg(\n",
    "    {**{c: \"mean\" for c in numeric_cols},\n",
    "     **{c: get_mode for c in cate_col},\n",
    "     **{c: \"max\" for c in first_cols}}\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc17137",
   "metadata": {},
   "source": [
    "### Masking & interpolate missing short timestamp gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26908cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = df['date_time'].min()\n",
    "end = df['date_time'].max()\n",
    "full_range = pd.date_range(start=start, end=end, freq=\"h\") \n",
    "\n",
    "df_full = df_grouped.set_index('date_time').reindex(full_range)\n",
    "df_full.index.name = 'date_time'\n",
    "\n",
    "# Đánh dấu dữ liệu gốc có tồn tại hay không\n",
    "original_exists = df.set_index('date_time').index\n",
    "df_full['timestep_mask'] = df_full.index.isin(original_exists).astype(int)\n",
    "\n",
    "cols_to_interp = [\"temp\", \"rain_1h\", \"snow_1h\", \"clouds_all\", \"traffic_volume\"]\n",
    "\n",
    "# 4.1 Mask cho khoảng trống nhỏ (Small Gaps)\n",
    "# Logic: Chỉ những chỗ nội suy được trong phạm vi SMALL_GAP mới là data dùng được\n",
    "mask_filled = np.ones(len(df_full), dtype=bool)\n",
    "\n",
    "for col in cols_to_interp:\n",
    "    mask_col = df_full[col].interpolate(\n",
    "        method=\"time\",\n",
    "        limit=CFG.SMALL_GAP,\n",
    "        limit_direction=\"forward\"\n",
    "    ).notna()\n",
    "\n",
    "    mask_filled &= mask_col   # chỉ giữ lại timestep valid ở tất cả các cột\n",
    "\n",
    "mask_filled = mask_filled.astype(int)\n",
    "\n",
    "# 4.2 Thực hiện Interpolate\n",
    "df_full[cols_to_interp] = df_full[cols_to_interp].interpolate(method='time', limit=CFG.SMALL_GAP, limit_direction='forward')\n",
    "df_full[cate_col] = df_full[cate_col].ffill(limit=CFG.SMALL_GAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9084f9ec",
   "metadata": {},
   "source": [
    "### Fill large timestamp gaps with 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97fcf33",
   "metadata": {},
   "source": [
    "Những chỗ Gap quá lớn (lớn hơn limit) sẽ vẫn còn NaN -> Fill bằng 0 hoặc bỏ qua khi train. Tuy nhiên để an toàn, ta fillna bằng phương pháp ffill/bfill nhẹ hoặc 0, nhưng mask = 0 sẽ chặn model học ở đó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944cfa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full['timestep_mask'] = mask_filled \n",
    "df_full[cols_to_interp] = df_full[cols_to_interp].fillna(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9400e0",
   "metadata": {},
   "source": [
    "### Interpolate is_holiday in small gaps by data of that day in other years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e09d00",
   "metadata": {},
   "source": [
    "Lấy danh sách ngày lễ từ dữ liệu gốc và gán lại cả ngày đó là lễ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4cf1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = df_full.reset_index()\n",
    "df_full['date_only'] = df_full['date_time'].dt.date\n",
    "\n",
    "holiday_dates = df_full[df_full['is_holiday'] == 1]['date_only'].dropna().unique()\n",
    "\n",
    "df_full['is_holiday'] = df_full['date_only'].isin(holiday_dates).astype(int)\n",
    "\n",
    "df_full = df_full.drop(columns=['date_only'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8041b6ad",
   "metadata": {},
   "source": [
    "## 6. Handle statiscal outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597bec0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full[\"rain_log\"] = np.log1p(df_full[\"rain_1h\"]) \n",
    "df_full[\"snow_log\"] = np.log1p(df_full[\"snow_1h\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0039c489",
   "metadata": {},
   "source": [
    "#  III. Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28931594",
   "metadata": {},
   "source": [
    "## 1. Encoding weather_description and drop weather_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11e7b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_idx = int(0.7 * len(df_full))\n",
    "train_cutoff_time = df_full.iloc[train_split_idx].name # Lấy timestamp tại điểm cắt\n",
    "\n",
    "# 2. Tạo bảng tham chiếu (Mapping Dictionary)\n",
    "train_subset = df_full[\n",
    "    (df_full.index <= train_cutoff_time) & \n",
    "    (df_full['timestep_mask'] == 1)\n",
    "]\n",
    "\n",
    "# Tính map: Mỗi loại thời tiết tương ứng với traffic trung bình là bao nhiêu?\n",
    "weather_score_map = train_subset.groupby('weather_description')['traffic_volume'].mean().to_dict()\n",
    "global_mean_score = train_subset['traffic_volume'].mean() # Điểm mặc định nếu gặp thời tiết lạ\n",
    "\n",
    "# 3. Áp dụng Map cho TOÀN BỘ df_full\n",
    "# Lúc này ta dùng cột 'weather_description' đã được ffill đầy đủ ở Stage 3\n",
    "df_full['weather_score'] = df_full['weather_description'].map(weather_score_map)\n",
    "\n",
    "# 4. Xử lý Missing sau khi Map\n",
    "# Nếu có loại thời tiết nào ở tập Test mà Train chưa từng thấy -> Fill bằng global_mean\n",
    "df_full['weather_score'] = df_full['weather_score'].fillna(global_mean_score)\n",
    "\n",
    "# Drop cột text gốc nếu không dùng nữa\n",
    "df_full = df_full.drop(columns=['weather_description', 'weather_main'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb2a2ef",
   "metadata": {},
   "source": [
    "## 2. Add time feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18b70a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full[\"year\"] = df_full[\"date_time\"].dt.year  # bỏ vì tăng dần vô hạn, khi đến tập test sẽ tạo ra khoảng max và tạo giá trị > 1\n",
    "df_full[\"month\"] = df_full[\"date_time\"].dt.month # bỏ vì có month sin cos\n",
    "df_full[\"day\"] = df_full[\"date_time\"].dt.day # bỏ vì nó discontinue (ví dụ ngày 31 và ngày 1 sát nhau nhưng về mặt con số cách xa nhau)\n",
    "df_full[\"hour\"] = df_full[\"date_time\"].dt.hour #bỏ vì có hour sin cos\n",
    "df_full[\"day_of_week\"] = df_full[\"date_time\"].dt.dayofweek # bỏ vì có dow sin cos \n",
    "df_full[\"is_weekend\"] = (df_full[\"day_of_week\"] >= 5).astype(int)\n",
    "df_full[\"is_rain\"] = (df_full[\"rain_1h\"] > 0).astype(int)\n",
    "df_full[\"is_snow\"] = (df_full[\"snow_1h\"] > 0).astype(int)\n",
    "\n",
    "# Cyclical encoding\n",
    "df_full[\"hour_sin\"] = np.sin(2*np.pi*df_full[\"hour\"]/24)\n",
    "df_full[\"hour_cos\"] = np.cos(2*np.pi*df_full[\"hour\"]/24)\n",
    "df_full[\"dow_sin\"] = np.sin(2*np.pi*df_full[\"day_of_week\"]/7)\n",
    "df_full[\"dow_cos\"] = np.cos(2*np.pi*df_full[\"day_of_week\"]/7)\n",
    "df_full[\"month_sin\"] = np.sin(2*np.pi*df_full[\"month\"]/12)\n",
    "df_full[\"month_cos\"] = np.cos(2*np.pi*df_full[\"month\"]/12)\n",
    "df_full[\"trend\"] = np.arange(len(df_full)) # bỏ vì tăng dần vô hạn, khi đến tập test sẽ tạo ra khoảng max và tạo giá trị >1\n",
    "\n",
    "week_hour = df_full['day_of_week']*24 + df_full['hour']\n",
    "df_full['sin_168'] = np.sin(2*np.pi*week_hour/168)\n",
    "df_full['cos_168'] = np.cos(2*np.pi*week_hour/168)\n",
    "df_full[\"fourier_sin_year\"] = np.sin(2*np.pi*df_full[\"trend\"]/(365*24))\n",
    "df_full[\"fourier_cos_year\"] = np.cos(2*np.pi*df_full[\"trend\"]/(365*24))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d10a011",
   "metadata": {},
   "source": [
    "#  IV. Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc91944",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    \"temp\",\"rain_log\",\"clouds_all\",\"snow_log\",\"is_holiday\",\"weather_score\",\n",
    "    \"is_weekend\",\"is_rain\",\"is_snow\",\n",
    "    \"hour_sin\",\"hour_cos\",\"dow_sin\",\"dow_cos\",\"month_sin\",\"month_cos\",\n",
    "    \"sin_168\",\"cos_168\",\"fourier_sin_year\",\"fourier_cos_year\"\n",
    "]\n",
    "\n",
    "X_all = df_full[feature_cols].values.astype(np.float32)\n",
    "y_all = df_full[['traffic_volume']].values.astype(np.float32)\n",
    "mask_all = df_full['timestep_mask'].values\n",
    "\n",
    "print(f\"NaN count in X_all: {np.isnan(X_all).sum()}\")\n",
    "print(f\"NaN count in y_all: {np.isnan(y_all).sum()}\")\n",
    "\n",
    "total_len = len(X_all)\n",
    "train_end_idx = int(0.7 * total_len)\n",
    "val_end_idx   = int(0.8 * total_len)\n",
    "\n",
    "X_train_raw = X_all[:train_end_idx]\n",
    "X_val_raw   = X_all[train_end_idx:val_end_idx]\n",
    "X_test_raw  = X_all[val_end_idx:]\n",
    "\n",
    "y_train_raw = y_all[:train_end_idx]\n",
    "y_val_raw   = y_all[train_end_idx:val_end_idx]\n",
    "y_test_raw  = y_all[val_end_idx:]\n",
    "\n",
    "mask_train = mask_all[:train_end_idx]\n",
    "mask_val   = mask_all[train_end_idx:val_end_idx]\n",
    "mask_test  = mask_all[val_end_idx:]\n",
    "\n",
    "# Fit ONLY on TRAIN \n",
    "train_valid_idx = mask_train == 1\n",
    "\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_X.fit(X_train_raw[train_valid_idx])  # ✅ Only valid rows\n",
    "\n",
    "scaler_y = MinMaxScaler()\n",
    "scaler_y.fit(y_train_raw[train_valid_idx])\n",
    "\n",
    "# Transform Train/Val/Test \n",
    "X_train = scaler_X.transform(X_train_raw)\n",
    "X_val   = scaler_X.transform(X_val_raw)\n",
    "X_test  = scaler_X.transform(X_test_raw)\n",
    "\n",
    "y_train = scaler_y.transform(y_train_raw)\n",
    "y_val   = scaler_y.transform(y_val_raw)\n",
    "y_test  = scaler_y.transform(y_test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f42858",
   "metadata": {},
   "source": [
    "# V. Data preparing for tran"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad443745",
   "metadata": {},
   "source": [
    "##  1. Dataset Break the Sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdda71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blocks(mask):\n",
    "    mask = np.array(mask)\n",
    "    padded = np.r_[0, mask, 0]\n",
    "    diff = np.diff(padded)\n",
    "    starts = np.where(diff == 1)[0]\n",
    "    ends = np.where(diff == -1)[0]\n",
    "    return list(zip(starts, ends))  \n",
    "\n",
    "class BreakSequenceDataset(Dataset):\n",
    "    def __init__(self, X, y, mask, lookback, horizon):\n",
    "        self.lookback = lookback\n",
    "        self.horizon = horizon\n",
    "        self.samples = []\n",
    "\n",
    "        X, y, mask = np.array(X), np.array(y), np.array(mask)\n",
    "        for start, end in get_blocks(mask):\n",
    "            L = end - start\n",
    "            if L < lookback + horizon:\n",
    "                continue\n",
    "            for i in range(start, end - lookback - horizon + 1):\n",
    "                x_seq = X[i:i+lookback]\n",
    "                y_seq = y[i+lookback:i+lookback+horizon]\n",
    "                self.samples.append((x_seq, y_seq))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.samples[idx]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y.reshape(-1), dtype=torch.float32)\n",
    "\n",
    "train_ds = BreakSequenceDataset(X_train, y_train, mask_train, CFG.lookback, CFG.horizon)\n",
    "val_ds   = BreakSequenceDataset(X_val, y_val, mask_val, CFG.lookback, CFG.horizon)\n",
    "test_ds  = BreakSequenceDataset(X_test, y_test, mask_test, CFG.lookback, CFG.horizon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb56b2b",
   "metadata": {},
   "source": [
    "## 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fdbb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=CFG.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=CFG.batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=CFG.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279ce4f8",
   "metadata": {},
   "source": [
    "#  VI. Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6e9b1d",
   "metadata": {},
   "source": [
    "## 1. Simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabc4a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden, horizon):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden, batch_first=True)\n",
    "        self.head = nn.Linear(hidden, horizon)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out_last = out[:, -1, :]\n",
    "        return self.head(out_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb628ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, train_loader, val_loader, epochs=6, lr=1e-3):\n",
    "    device = torch.device(CFG.device)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    best_val = float('inf')\n",
    "    best_state = None\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                pred = model(xb)\n",
    "                val_losses.append(criterion(pred, yb).item())\n",
    "        avg_tr, avg_val = np.mean(train_losses), np.mean(val_losses)\n",
    "        if avg_val < best_val:\n",
    "            best_val = avg_val\n",
    "            best_state = model.state_dict()\n",
    "        print(f'Epoch {ep}/{epochs} train_loss={avg_tr:.6f} val_loss={avg_val:.6f}')\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c1c358",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99522b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, scaler_y, eps=1e-3):\n",
    "    device = torch.device(CFG.device)\n",
    "    model.eval()\n",
    "\n",
    "    Ys = []\n",
    "    Ps = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            pred = model(xb)        \n",
    "            Ys.append(yb.numpy())     \n",
    "            Ps.append(pred.cpu().numpy())\n",
    "\n",
    "    Y = np.vstack(Ys)                \n",
    "    P = np.vstack(Ps)                \n",
    "\n",
    "    Y_inv = scaler_y.inverse_transform(Y)\n",
    "    P_inv = scaler_y.inverse_transform(P)\n",
    "\n",
    "    mae  = mean_absolute_error(Y_inv, P_inv)\n",
    "    rmse = np.sqrt(mean_squared_error(Y_inv, P_inv))\n",
    "    r2 = r2_score(Y_inv, P_inv)\n",
    "    #NSE \n",
    "    numerator = np.sum((Y_inv - P_inv)**2)\n",
    "    denominator = np.sum((Y_inv - np.mean(Y_inv))**2) + eps\n",
    "    nse = 1 - numerator / denominator\n",
    "\n",
    "\n",
    "    metrics = {\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"NSE\": nse,\n",
    "        \"R2\": r2\n",
    "    }\n",
    "    return metrics, Y_inv, P_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2c183d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleLSTM(input_dim=X_train.shape[1], hidden=128, horizon=CFG.horizon)\n",
    "model = train_loop(model, train_loader, val_loader, epochs=CFG.epochs, lr=CFG.lr)\n",
    "\n",
    "metrics_train, Y_train_inv, P_train_inv = evaluate_model(model, train_loader, scaler_y)\n",
    "metrics_val,   Y_val_inv,   P_val_inv   = evaluate_model(model, val_loader, scaler_y)   \n",
    "\n",
    "print(\"Train Metrics:\", metrics_train)\n",
    "print(\"Val Metrics:\", metrics_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82aa7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_horizon2(Y, P, chart_name, max_points=300):\n",
    "    N = min(len(Y), max_points)\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.plot(Y[:N, 1], label='Actual (H2)')\n",
    "    plt.plot(P[:N, 1], label='Predict (H2)')\n",
    "    plt.title(f'{chart_name}')\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "metrics, Y_inv, P_inv = evaluate_model(model, test_loader, scaler_y)\n",
    "\n",
    "plot_horizon2(Y_train_inv, P_train_inv, 'train')\n",
    "plot_horizon2(Y_val_inv, P_val_inv, 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd523b83",
   "metadata": {},
   "source": [
    "### Hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3088eb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 32, 256)\n",
    "    lr          = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    batch_size  = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "\n",
    "    # DataLoaders \n",
    "    train_loader_opt = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader_opt   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Model \n",
    "    model = SimpleLSTM(input_dim=X_train.shape[1], hidden=hidden_size, horizon=CFG.horizon).to(CFG.device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    EPOCHS_OPT = 6\n",
    "    for ep in range(EPOCHS_OPT):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader_opt:\n",
    "            xb, yb = xb.to(CFG.device), yb.to(CFG.device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation \n",
    "        model.eval()\n",
    "        Ys, Ps = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader_opt:\n",
    "                xb = xb.to(CFG.device)\n",
    "                pred = model(xb)\n",
    "                Ys.append(yb.cpu().numpy())\n",
    "                Ps.append(pred.cpu().numpy())\n",
    "\n",
    "        Y_val = np.vstack(Ys)\n",
    "        P_val = np.vstack(Ps)\n",
    "\n",
    "        # Inverse scaling\n",
    "        Y_val_inv = scaler_y.inverse_transform(Y_val)\n",
    "        P_val_inv = scaler_y.inverse_transform(P_val)\n",
    "\n",
    "        val_mae = mean_absolute_error(Y_val_inv, P_val_inv)\n",
    "\n",
    "        trial.report(val_mae, ep)\n",
    "\n",
    "        if val_mae < best_val_loss:\n",
    "            best_val_loss = val_mae\n",
    "\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20) \n",
    "\n",
    "print(\"Best trial hyperparams:\")\n",
    "print(study.best_trial.params)\n",
    "print(\"Best MAE:\", study.best_trial.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539ff8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_trial.params\n",
    "hidden_size = best_params[\"hidden_size\"]\n",
    "lr          = best_params[\"lr\"]\n",
    "batch_size  = best_params[\"batch_size\"]\n",
    "\n",
    "train_loader_best = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader_best   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "test_loader_best  = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model final\n",
    "model_best = SimpleLSTM(input_dim=X_train.shape[1], hidden=hidden_size, horizon=CFG.horizon)\n",
    "model_best = train_loop(model_best, train_loader_best, val_loader_best, epochs=CFG.epochs, lr=lr)\n",
    "\n",
    "# Evaluate Test\n",
    "metrics_test, Y_test_inv, P_test_inv = evaluate_model(model_best, test_loader_best, scaler_y)\n",
    "print(\"\\nTest Metrics with best params:\")\n",
    "print(metrics_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efa319d",
   "metadata": {},
   "source": [
    "## 2. Encode - Decoder LSTM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
